#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.8"
# ///

import argparse
import json
import sys
from datetime import datetime
from pathlib import Path

# Import our notification utility
sys.path.insert(0, str(Path(__file__).parent / "utils"))
from notify import log_notification_event, send_notification

def log_subagent_output(input_data: dict):
    """
    Log detailed subagent output to structured files.
    
    Args:
        input_data: Hook input data containing tool information and output
    """
    try:
        # Extract relevant data
        tool_name = input_data.get('tool_name', '')
        tool_input = input_data.get('tool_input', {})
        tool_response = input_data.get('tool_response', {})
        timestamp = datetime.now()
        
        # Only log Task tool calls (subagent invocations)
        if tool_name != 'Task':
            return
            
        # Extract subagent information
        subagent_type = tool_input.get('subagent_type', 'unknown')
        description = tool_input.get('description', 'No description')
        prompt = tool_input.get('prompt', '')
        
        # Extract response information - handle different response formats
        result = ''
        duration_ms = tool_response.get('totalDurationMs', 0)
        tokens = tool_response.get('totalTokens', 0)
        tool_use_count = tool_response.get('totalToolUseCount', 0)
        usage = tool_response.get('usage', {})
        
        # Check for content array format (Task agent responses)
        content = tool_response.get('content', [])
        if content and isinstance(content, list) and len(content) > 0:
            # Extract text from first content item
            first_content = content[0]
            if isinstance(first_content, dict) and 'text' in first_content:
                result = first_content['text']
        
        # Fallback to simple result field
        if not result:
            result = tool_response.get('result', '')
        
        # Create log directories
        log_dir = Path.cwd() / '.logs' / 'subagents'
        log_dir.mkdir(parents=True, exist_ok=True)
        
        # Date-based organization
        date_str = timestamp.strftime('%Y-%m-%d')
        daily_dir = log_dir / date_str
        daily_dir.mkdir(parents=True, exist_ok=True)
        
        # Generate unique filename
        time_str = timestamp.strftime('%H-%M-%S')
        session_id = input_data.get('session_id', 'unknown')[:8]
        filename_base = f"{time_str}_{subagent_type}_{session_id}"
        
        # Create structured JSON log
        json_file = daily_dir / f"{filename_base}.json"
        log_entry = {
            "timestamp": timestamp.isoformat(),
            "session_id": input_data.get('session_id', 'unknown'),
            "subagent_type": subagent_type,
            "description": description,
            "prompt": prompt,
            "result": result,
            "performance": {
                "duration_ms": duration_ms,
                "total_tokens": tokens,
                "tool_use_count": tool_use_count,
                "usage": usage
            },
            "tool_input": tool_input,
            "tool_response": tool_response
        }
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(log_entry, f, indent=2, ensure_ascii=False)
        
        # Create human-readable markdown log
        md_file = daily_dir / f"{filename_base}.md"
        
        # Format performance stats
        perf_section = ""
        if duration_ms or tokens or tool_use_count:
            perf_section = f"""
## Performance Stats
- **Duration:** {duration_ms:,}ms ({duration_ms/1000:.1f}s)
- **Tokens:** {tokens:,} total
- **Tool Uses:** {tool_use_count}
"""
            if usage:
                input_tokens = usage.get('input_tokens', 0)
                output_tokens = usage.get('output_tokens', 0) 
                if input_tokens or output_tokens:
                    perf_section += f"- **Token Breakdown:** {input_tokens:,} input, {output_tokens:,} output\n"
        
        md_content = f"""# Subagent Output Log

**Date:** {timestamp.strftime('%Y-%m-%d %H:%M:%S')}
**Subagent Type:** {subagent_type}
**Description:** {description}
**Session ID:** {session_id}

## Task Prompt
```
{prompt}
```
{perf_section}
## Result
```
{result}
```

---
*Generated by subagent_output_logger.py*
"""
        
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(md_content)
        
        # Update summary log
        summary_file = log_dir / 'summary.json'
        update_summary_log(summary_file, {
            "timestamp": timestamp.isoformat(),
            "subagent_type": subagent_type,
            "description": description,
            "filename": filename_base,
            "result_length": len(result)
        })
        
    except Exception as e:
        # Log error but don't fail the hook
        error_log = Path.cwd() / '.logs' / 'hook_errors.log'
        error_log.parent.mkdir(parents=True, exist_ok=True)
        with open(error_log, 'a', encoding='utf-8') as f:
            f.write(f"[{datetime.now().isoformat()}] subagent_output_logger error: {str(e)}\n")

def update_summary_log(summary_file: Path, entry: dict):
    """Update the summary log with a new entry."""
    try:
        # Read existing summary
        if summary_file.exists():
            with open(summary_file, 'r', encoding='utf-8') as f:
                summary_data = json.load(f)
        else:
            summary_data = {
                "total_subagent_calls": 0,
                "by_type": {},
                "recent_entries": []
            }
        
        # Update counters
        summary_data["total_subagent_calls"] += 1
        subagent_type = entry["subagent_type"]
        if subagent_type not in summary_data["by_type"]:
            summary_data["by_type"][subagent_type] = 0
        summary_data["by_type"][subagent_type] += 1
        
        # Add to recent entries (keep last 50)
        summary_data["recent_entries"].insert(0, entry)
        summary_data["recent_entries"] = summary_data["recent_entries"][:50]
        
        # Write back
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary_data, f, indent=2, ensure_ascii=False)
            
    except Exception:
        # Silent fail for summary updates
        pass

def main():
    try:
        # Parse command line arguments
        parser = argparse.ArgumentParser(description='Log subagent outputs')
        parser.add_argument('--quiet', action='store_true',
                            help='Suppress notifications (log only)')
        args = parser.parse_args()
        
        # Read JSON input from stdin
        input_data = json.load(sys.stdin)
        
        # Log the subagent output
        log_subagent_output(input_data)
        
        # Always exit successfully to not interrupt Claude's workflow
        sys.exit(0)
        
    except json.JSONDecodeError:
        # Handle JSON decode errors gracefully
        sys.exit(0)
    except Exception:
        # Handle any other errors gracefully
        sys.exit(0)

if __name__ == '__main__':
    main()